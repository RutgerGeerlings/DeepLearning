{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_log_training_time(modelname, start_time, end_time):\n",
    "    training_duration = end_time - start_time\n",
    "    # Print training duration in a human-readable format\n",
    "    hours, rem = divmod(training_duration, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    filepath = modelname+\"trainingtime.txt\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(f\"Training took {int(hours):02d}:{int(minutes):02d}:{seconds:02f} (hh:mm:ss).\")\n",
    "\n",
    "\n",
    "\n",
    "## runtime\n",
    "def save_model_config_with_optimizer(model, modelname):\n",
    "    filepath = modelname+\"model_config.txt\"\n",
    "    with open(filepath, 'w') as f:\n",
    "        # Save layer configurations\n",
    "        for layer in model.layers:\n",
    "            f.write(f\"Layer: {layer.name}\\n\")\n",
    "            f.write(f\"Config: {layer.get_config()}\\n\\n\")\n",
    "        \n",
    "        # Save optimizer configuration\n",
    "        optimizer_config = model.optimizer.get_config()\n",
    "        f.write(\"Optimizer Config:\\n\")\n",
    "        f.write(str(optimizer_config))\n",
    "\n",
    "\n",
    "def save_model(model, modelname):\n",
    "    model.save(modelname+\".keras\")\n",
    "\n",
    "def save_performance_metrics(history, modelname):\n",
    "    filepath = modelname+'performance_metrics.csv'\n",
    "    pd.DataFrame(history.history).to_csv(filepath)\n",
    "    \n",
    "    \n",
    "        \n",
    "def plot_loss_and_metric(history, metric_name='accuracy', model_name='model'):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss, and a performance metric from the training history, and saves the plots\n",
    "    with the model name as a prefix. This version explicitly uses Figures and Axes for better control.\n",
    "\n",
    "    Parameters:\n",
    "    - history: Return value from model.fit().\n",
    "    - metric_name: Name of the performance metric to plot (e.g., 'accuracy', 'mae').\n",
    "    - model_name: Name of the model, used as a prefix for saving plot images.\n",
    "    \"\"\"\n",
    "\n",
    "    # First plot: Training & validation loss values\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))  # Explicitly create a Figure and Axes\n",
    "    ax.plot(history.history['loss'], label='Train Loss')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax.set_title('Model Loss')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True)  # Adding grid explicitly\n",
    "    loss_plot_filename = f'{model_name}_loss_plot.jpg'\n",
    "    fig.savefig(loss_plot_filename)\n",
    "    print(f\"Loss plot saved as: {loss_plot_filename}\")\n",
    "    plt.close(fig)  # Close the figure\n",
    "\n",
    "    # Second plot: Training & validation performance metric\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))  # Again, explicitly create a Figure and Axes for the metric plot\n",
    "    ax.plot(history.history[metric_name], label=f'Train {metric_name.capitalize()}')\n",
    "    ax.plot(history.history[f'val_{metric_name}'], label=f'Validation {metric_name.capitalize()}')\n",
    "    ax.set_title(f'Model {metric_name.capitalize()}')\n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True)  # Adding grid explicitly\n",
    "    metric_plot_filename = f'{model_name}_{metric_name}_plot.jpg'\n",
    "    fig.savefig(metric_plot_filename)\n",
    "    print(f\"Metric plot saved as: {metric_plot_filename}\")\n",
    "    plt.close(fig)  # Close the figure to free up memory\n",
    "\n",
    "\n",
    "def save_model_and_config_and_metrics(model, history, modelname = \"model\"):\n",
    "    save_model_config_with_optimizer(model, modelname = modelname)\n",
    "    save_model(model, modelname = modelname)\n",
    "    save_performance_metrics(history, modelname = modelname)\n",
    "    plot_loss_and_metric(history, metric_name='mae', model_name= modelname)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def calculate_and_log_training_time(modelname, start_time, end_time):\n",
    "    training_duration = end_time - start_time\n",
    "    hours, rem = divmod(training_duration, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    model_dir = modelname\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    filepath = os.path.join(model_dir, \"trainingtime.txt\")\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(f\"Training took {int(hours):02d}:{int(minutes):02d}:{seconds:02f} (hh:mm:ss).\")\n",
    "\n",
    "def save_model_config_with_optimizer(model, modelname):\n",
    "    model_dir = modelname\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    filepath = os.path.join(model_dir, \"model_config.txt\")\n",
    "    with open(filepath, 'w') as f:\n",
    "        for layer in model.layers:\n",
    "            f.write(f\"Layer: {layer.name}\\n\")\n",
    "            f.write(f\"Config: {layer.get_config()}\\n\\n\")\n",
    "        \n",
    "        optimizer_config = model.optimizer.get_config()\n",
    "        f.write(\"Optimizer Config:\\n\")\n",
    "        f.write(str(optimizer_config))\n",
    "\n",
    "def save_model(model, modelname):\n",
    "    model_dir = modelname\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    model_path = os.path.join(model_dir, modelname + \".keras\")\n",
    "    model.save(model_path)\n",
    "\n",
    "def save_performance_metrics(history, modelname):\n",
    "    model_dir = modelname\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    filepath = os.path.join(model_dir, 'performance_metrics.csv')\n",
    "    pd.DataFrame(history.history).to_csv(filepath)\n",
    "\n",
    "def plot_loss_and_metric(history, metric_name='accuracy', model_name='model'):\n",
    "    model_dir = model_name\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # First plot: Training & validation loss\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(history.history['loss'], label='Train Loss')\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax.set_title('Model Loss')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True)\n",
    "    loss_plot_filename = os.path.join(model_dir, f'{model_name}_loss_plot.jpg')\n",
    "    fig.savefig(loss_plot_filename)\n",
    "    print(f\"Loss plot saved as: {loss_plot_filename}\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Second plot: Training & validation metric\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(history.history[metric_name], label=f'Train {metric_name.capitalize()}')\n",
    "    ax.plot(history.history[f'val_{metric_name}'], label=f'Validation {metric_name.capitalize()}')\n",
    "    ax.set_title(f'Model {metric_name.capitalize()}')\n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True)\n",
    "    metric_plot_filename = os.path.join(model_dir, f'{model_name}_{metric_name}_plot.jpg')\n",
    "    fig.savefig(metric_plot_filename)\n",
    "    print(f\"Metric plot saved as: {metric_plot_filename}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def save_model_and_config_and_metrics(model, history, modelname = \"model\"):\n",
    "    save_model_config_with_optimizer(model, modelname = modelname)\n",
    "    save_model(model, modelname = modelname)\n",
    "    save_performance_metrics(history, modelname = modelname)\n",
    "    plot_loss_and_metric(history, metric_name='mae', model_name= modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\busjo\\Documents\\JADS\\Semester 2\\Deep Learning\\Project\\Part1_Processed_Grey.pkl\"\n",
    "# Open the pickle file in binary mode\n",
    "with open(path, 'rb') as file:\n",
    "    # Load the content of the file into a variable\n",
    "    grey_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_grey_data = grey_data.sample(n=1500, random_state = 2001)\n",
    "#sampled_grey_data = grey_data\n",
    "\n",
    "X = np.stack(sampled_grey_data['Image'].values)  # Converts a column of arrays into a single numpy array\n",
    "X = X.reshape(X.shape[0], 256, 256, 1)\n",
    "y = sampled_grey_data['Age'].values\n",
    "#X = list(sampled_grey_data['Image'].values)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\busjo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model1.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "\n",
    "modelname = 'cnn_grey_1'\n",
    "calculate_and_log_training_time(modelname = modelname, start_time = start_time, end_time = end_time)\n",
    "save_model_and_config_and_metrics(model = model1, history = history,  modelname = modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\busjo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 935ms/step - loss: 1634.1606 - mae: 30.7707 - val_loss: 883.7107 - val_mae: 22.2792\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 926ms/step - loss: 825.7773 - mae: 23.5434 - val_loss: 783.1424 - val_mae: 21.3620\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 929ms/step - loss: 726.5474 - mae: 22.3873 - val_loss: 746.2041 - val_mae: 20.9406\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 975ms/step - loss: 669.0268 - mae: 21.5673 - val_loss: 774.5165 - val_mae: 21.0245\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 937ms/step - loss: 695.4950 - mae: 21.8337 - val_loss: 761.0788 - val_mae: 20.8424\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 964ms/step - loss: 709.5378 - mae: 22.1661 - val_loss: 692.1946 - val_mae: 20.2734\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 664.1293 - mae: 21.4943 - val_loss: 703.6348 - val_mae: 20.2127\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step - loss: 615.4731 - mae: 20.0641 - val_loss: 693.1008 - val_mae: 19.5775\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step - loss: 593.4595 - mae: 19.9728 - val_loss: 570.6383 - val_mae: 18.9788\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 942ms/step - loss: 537.8315 - mae: 19.0495 - val_loss: 574.9476 - val_mae: 17.9459\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 944ms/step - loss: 535.9387 - mae: 18.2951 - val_loss: 507.4341 - val_mae: 16.9793\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 928ms/step - loss: 478.8901 - mae: 17.2914 - val_loss: 463.0729 - val_mae: 16.8051\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 925ms/step - loss: 509.0442 - mae: 17.5084 - val_loss: 576.4334 - val_mae: 17.2271\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 926ms/step - loss: 422.2125 - mae: 15.7401 - val_loss: 502.7821 - val_mae: 16.4476\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 930ms/step - loss: 404.9285 - mae: 15.4128 - val_loss: 470.4867 - val_mae: 16.6458\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 936ms/step - loss: 380.1203 - mae: 14.9630 - val_loss: 520.2084 - val_mae: 16.4472\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 933ms/step - loss: 401.7313 - mae: 15.0298 - val_loss: 507.5545 - val_mae: 16.2165\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 936ms/step - loss: 375.8576 - mae: 14.4073 - val_loss: 468.6047 - val_mae: 15.2247\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 937ms/step - loss: 380.1524 - mae: 14.3690 - val_loss: 470.1224 - val_mae: 16.3571\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 934ms/step - loss: 348.7084 - mae: 14.2243 - val_loss: 535.5958 - val_mae: 16.3733\n",
      "Loss plot saved as: cnn_grey_2_dropout_loss_plot.jpg\n",
      "Metric plot saved as: cnn_grey_2_dropout_mae_plot.jpg\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "model2.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model2.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "\n",
    "modelname = 'cnn_grey_2_dropout'\n",
    "calculate_and_log_training_time(modelname = modelname, start_time = start_time, end_time = end_time)\n",
    "save_model_and_config_and_metrics(model = model2, history = history,  modelname = modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\busjo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 913ms/step - loss: 1127.5836 - mae: 26.1343 - val_loss: 660.0602 - val_mae: 21.3629\n",
      "Epoch 2/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 907ms/step - loss: 724.4761 - mae: 22.4172 - val_loss: 665.9376 - val_mae: 20.7162\n",
      "Epoch 3/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 960ms/step - loss: 623.8688 - mae: 20.9278 - val_loss: 601.1034 - val_mae: 19.3597\n",
      "Epoch 4/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 962ms/step - loss: 574.1974 - mae: 19.7684 - val_loss: 548.5363 - val_mae: 20.1795\n",
      "Epoch 5/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 911ms/step - loss: 510.9729 - mae: 18.2532 - val_loss: 426.5843 - val_mae: 16.7738\n",
      "Epoch 6/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 897ms/step - loss: 494.5172 - mae: 17.7932 - val_loss: 437.5464 - val_mae: 16.4820\n",
      "Epoch 7/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 916ms/step - loss: 418.7695 - mae: 16.2503 - val_loss: 420.2061 - val_mae: 14.8697\n",
      "Epoch 8/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 899ms/step - loss: 332.6356 - mae: 14.0505 - val_loss: 404.6701 - val_mae: 14.6636\n",
      "Epoch 9/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 860ms/step - loss: 308.9039 - mae: 13.5592 - val_loss: 412.6090 - val_mae: 14.7276\n",
      "Epoch 10/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 839ms/step - loss: 335.1396 - mae: 14.1511 - val_loss: 425.9492 - val_mae: 14.8297\n",
      "Epoch 11/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 848ms/step - loss: 305.9641 - mae: 13.0410 - val_loss: 419.2534 - val_mae: 15.6741\n",
      "Epoch 12/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 856ms/step - loss: 238.0679 - mae: 11.7625 - val_loss: 341.6553 - val_mae: 13.4356\n",
      "Epoch 13/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 860ms/step - loss: 198.8976 - mae: 10.4601 - val_loss: 343.1010 - val_mae: 13.1412\n",
      "Epoch 14/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 864ms/step - loss: 173.3530 - mae: 9.7964 - val_loss: 356.4335 - val_mae: 14.2317\n",
      "Epoch 15/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 875ms/step - loss: 170.8510 - mae: 9.6781 - val_loss: 352.5517 - val_mae: 13.5426\n",
      "Epoch 16/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 855ms/step - loss: 133.5957 - mae: 8.3593 - val_loss: 356.7854 - val_mae: 14.1341\n",
      "Epoch 17/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 852ms/step - loss: 120.4606 - mae: 7.6238 - val_loss: 347.3340 - val_mae: 13.6566\n",
      "Epoch 18/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 853ms/step - loss: 97.6988 - mae: 7.1432 - val_loss: 371.8388 - val_mae: 13.8247\n",
      "Epoch 19/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 852ms/step - loss: 89.2407 - mae: 6.6663 - val_loss: 344.1259 - val_mae: 13.5707\n",
      "Epoch 20/20\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 850ms/step - loss: 68.6153 - mae: 5.7434 - val_loss: 363.9824 - val_mae: 13.6407\n",
      "Loss plot saved as: cnn_grey_3_regularization\\cnn_grey_3_regularization_loss_plot.jpg\n",
      "Metric plot saved as: cnn_grey_3_regularization\\cnn_grey_3_regularization_mae_plot.jpg\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 167ms/step - loss: 396.7595 - mae: 13.6929\n",
      "Test Loss: 423.4404602050781\n",
      "Test MAE: 14.287540435791016\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1), kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model3.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model3.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "\n",
    "modelname = 'cnn_grey_3_regularization'\n",
    "calculate_and_log_training_time(modelname = modelname, start_time = start_time, end_time = end_time)\n",
    "save_model_and_config_and_metrics(model = model3, history = history,  modelname = modelname)\n",
    "\n",
    "test_loss, test_mae = model3.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\busjo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 680ms/step - loss: 1013.4998 - mae: 25.3227 - val_loss: 1001.5715 - val_mae: 23.1213\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 684ms/step - loss: 802.5677 - mae: 23.4325 - val_loss: 907.3276 - val_mae: 22.2427\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 672ms/step - loss: 658.5303 - mae: 21.1299 - val_loss: 658.4299 - val_mae: 20.5741\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 672ms/step - loss: 655.4486 - mae: 21.7516 - val_loss: 662.7924 - val_mae: 22.1545\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 675ms/step - loss: 674.6443 - mae: 21.7208 - val_loss: 567.0229 - val_mae: 18.4125\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 674ms/step - loss: 587.7359 - mae: 19.5968 - val_loss: 472.2002 - val_mae: 17.1843\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - loss: 498.4842 - mae: 17.8074 - val_loss: 499.5736 - val_mae: 18.5276\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - loss: 460.9418 - mae: 16.5977 - val_loss: 376.0168 - val_mae: 15.5668\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - loss: 387.6225 - mae: 15.3995 - val_loss: 473.2575 - val_mae: 15.8956\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - loss: 493.2173 - mae: 16.8742 - val_loss: 404.2415 - val_mae: 14.4927\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 679ms/step - loss: 341.9249 - mae: 14.2615 - val_loss: 330.8463 - val_mae: 14.2771\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - loss: 272.0178 - mae: 12.3907 - val_loss: 339.3008 - val_mae: 13.7272\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - loss: 262.7645 - mae: 12.1256 - val_loss: 377.8724 - val_mae: 14.8797\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 663ms/step - loss: 242.1410 - mae: 11.6733 - val_loss: 335.3111 - val_mae: 13.7008\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 665ms/step - loss: 239.5900 - mae: 11.6004 - val_loss: 326.9610 - val_mae: 12.6828\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - loss: 220.6516 - mae: 10.6052 - val_loss: 346.0345 - val_mae: 13.9875\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 671ms/step - loss: 171.9439 - mae: 9.6601 - val_loss: 358.6627 - val_mae: 13.7723\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - loss: 142.0279 - mae: 8.6589 - val_loss: 361.1789 - val_mae: 13.9229\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 662ms/step - loss: 158.2860 - mae: 9.0876 - val_loss: 354.2048 - val_mae: 13.9614\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 665ms/step - loss: 159.2594 - mae: 9.0485 - val_loss: 385.4804 - val_mae: 13.8654\n",
      "Loss plot saved as: cnn_grey_4_regularization_dropout\\cnn_grey_4_regularization_dropout_loss_plot.jpg\n",
      "Metric plot saved as: cnn_grey_4_regularization_dropout\\cnn_grey_4_regularization_dropout_mae_plot.jpg\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 169ms/step - loss: 456.3106 - mae: 15.8662\n",
      "Test Loss: 506.37078857421875\n",
      "Test MAE: 16.611051559448242\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1), kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.50),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model4.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model4.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "\n",
    "modelname = 'cnn_grey_4_regularization_dropout'\n",
    "calculate_and_log_training_time(modelname = modelname, start_time = start_time, end_time = end_time)\n",
    "save_model_and_config_and_metrics(model = model4, history = history,  modelname = modelname)\n",
    "\n",
    "test_loss, test_mae = model4.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\busjo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 680ms/step - loss: 1013.4998 - mae: 25.3227 - val_loss: 1001.5715 - val_mae: 23.1213\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 684ms/step - loss: 802.5677 - mae: 23.4325 - val_loss: 907.3276 - val_mae: 22.2427\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 672ms/step - loss: 658.5303 - mae: 21.1299 - val_loss: 658.4299 - val_mae: 20.5741\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 672ms/step - loss: 655.4486 - mae: 21.7516 - val_loss: 662.7924 - val_mae: 22.1545\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 675ms/step - loss: 674.6443 - mae: 21.7208 - val_loss: 567.0229 - val_mae: 18.4125\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 674ms/step - loss: 587.7359 - mae: 19.5968 - val_loss: 472.2002 - val_mae: 17.1843\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - loss: 498.4842 - mae: 17.8074 - val_loss: 499.5736 - val_mae: 18.5276\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - loss: 460.9418 - mae: 16.5977 - val_loss: 376.0168 - val_mae: 15.5668\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - loss: 387.6225 - mae: 15.3995 - val_loss: 473.2575 - val_mae: 15.8956\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - loss: 493.2173 - mae: 16.8742 - val_loss: 404.2415 - val_mae: 14.4927\n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 679ms/step - loss: 341.9249 - mae: 14.2615 - val_loss: 330.8463 - val_mae: 14.2771\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 670ms/step - loss: 272.0178 - mae: 12.3907 - val_loss: 339.3008 - val_mae: 13.7272\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 666ms/step - loss: 262.7645 - mae: 12.1256 - val_loss: 377.8724 - val_mae: 14.8797\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 663ms/step - loss: 242.1410 - mae: 11.6733 - val_loss: 335.3111 - val_mae: 13.7008\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 665ms/step - loss: 239.5900 - mae: 11.6004 - val_loss: 326.9610 - val_mae: 12.6828\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 667ms/step - loss: 220.6516 - mae: 10.6052 - val_loss: 346.0345 - val_mae: 13.9875\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 671ms/step - loss: 171.9439 - mae: 9.6601 - val_loss: 358.6627 - val_mae: 13.7723\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 676ms/step - loss: 142.0279 - mae: 8.6589 - val_loss: 361.1789 - val_mae: 13.9229\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 662ms/step - loss: 158.2860 - mae: 9.0876 - val_loss: 354.2048 - val_mae: 13.9614\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 665ms/step - loss: 159.2594 - mae: 9.0485 - val_loss: 385.4804 - val_mae: 13.8654\n",
      "Loss plot saved as: cnn_grey_4_regularization_dropout\\cnn_grey_4_regularization_dropout_loss_plot.jpg\n",
      "Metric plot saved as: cnn_grey_4_regularization_dropout\\cnn_grey_4_regularization_dropout_mae_plot.jpg\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 169ms/step - loss: 456.3106 - mae: 15.8662\n",
      "Test Loss: 506.37078857421875\n",
      "Test MAE: 16.611051559448242\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1), kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.50),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model5.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "start_time = time.time()\n",
    "history = model5.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)\n",
    "end_time = time.time()\n",
    "\n",
    "modelname = 'cnn_grey_5_regularization_dropout'\n",
    "calculate_and_log_training_time(modelname = modelname, start_time = start_time, end_time = end_time)\n",
    "save_model_and_config_and_metrics(model = model5, history = history,  modelname = modelname)\n",
    "\n",
    "test_loss, test_mae = model5.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
